#!/usr/bin/env python3
"""
TechNews Auto-Publisher
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Fetches latest tech/AI/science news â†’ generates article via Groq AI (Llama 3.3)
â†’ publishes to MongoDB via your existing /api/posts endpoint
â†’ regenerates sitemap.xml
â†’ pings Google + submits via IndexNow (no account/card needed)
"""

import json
import logging
import datetime
from pathlib import Path
from StudyMaterial.news_bot.config import CONFIG
from StudyMaterial.news_bot.fetcher import fetch_news
from StudyMaterial.news_bot.generator import generate_article
from StudyMaterial.news_bot.publisher import publish_article
from StudyMaterial.news_bot.sitemap import generate_sitemap
from StudyMaterial.news_bot.indexer import submit_to_google

# â”€â”€ Logging â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Path("logs").mkdir(exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler("logs/run.log"),
        logging.StreamHandler()
    ]
)
log = logging.getLogger(__name__)


def load_published_slugs() -> set:
    path = Path("data/published.json")
    if path.exists():
        return set(json.loads(path.read_text()))
    return set()


def save_published_slug(slug: str):
    Path("data").mkdir(exist_ok=True)
    path = Path("data/published.json")
    slugs = load_published_slugs()
    slugs.add(slug)
    path.write_text(json.dumps(list(slugs), indent=2))


def run():
    log.info("=" * 60)
    log.info("ğŸš€ TechNews Auto-Publisher â€” Daily Run")
    log.info(f"   {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')} UTC")
    log.info("=" * 60)

    Path("data").mkdir(exist_ok=True)
    Path("output").mkdir(exist_ok=True)

    published = load_published_slugs()

    # â”€â”€ 1. Fetch News â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    log.info("ğŸ“¡ Step 1: Fetching latest news...")
    articles = fetch_news()
    if not articles:
        log.warning("No articles fetched. Exiting.")
        return
    log.info(f"   Found {len(articles)} relevant articles")

    # â”€â”€ 2. Generate Article via Groq AI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    log.info("âœï¸  Step 2: Generating article via Groq (llama-3.3-70b-versatile)...")
    result = generate_article(articles)
    if not result:
        log.error("Article generation failed. Exiting.")
        return

    slug = result["slug"]
    if slug in published:
        log.warning(f"Article '{slug}' already published today. Skipping.")
        return

    # â”€â”€ 3. Publish to MongoDB â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    log.info("ğŸŒ Step 3: Publishing to MongoDB via /api/posts...")
    published_url = publish_article(result)
    if not published_url:
        log.error("Publishing failed. Exiting.")
        return

    log.info(f"   âœ… Live at: {published_url}")
    save_published_slug(slug)

    # â”€â”€ 4. Regenerate Sitemap â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    log.info("ğŸ—ºï¸  Step 4: Regenerating sitemap.xml...")
    sitemap_path = generate_sitemap(new_slug=slug)
    log.info(
        f"   âœ… sitemap.xml saved to {sitemap_path}\n"
        f"   ğŸ“‹ Upload this file to your site root so it's live at:\n"
        f"      {CONFIG['site']['base_url']}/sitemap.xml"
    )

    # â”€â”€ 5. Submit for Indexing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    log.info("ğŸ” Step 5: Submitting to search engines...")
    submit_to_google(published_url)

    log.info("=" * 60)
    log.info("âœ… All done!")
    log.info("=" * 60)


if __name__ == "__main__":
    run()